{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e17613be-0a86-4b1a-9e95-cddeea9a26ae",
   "metadata": {},
   "source": [
    "## Backpropagation (오차역전파)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91decc3-3781-487f-892a-d2efe0dd3dce",
   "metadata": {},
   "source": [
    "## 5.4.1 곱셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdded74b-3bab-48c1-be42-3277396a460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "    \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y # x와y를 바꾼다.\n",
    "        dy = dout * self.x \n",
    "\n",
    "        return dx, dy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0eab0f-a9f7-4138-9fd3-38ac93357c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(int(price))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9cb2e-dfed-4a14-9e2d-442d07764115",
   "metadata": {},
   "source": [
    "## 5.4.2 덧셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc3e74c4-b394-4aad-8242-ade3653ff632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7e39da-a98b-4e98-b2db-d2a660315469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715\n",
      "dall_price ->  1.1\n",
      "dtax ->  650\n",
      "dapple_price ->  1.1\n",
      "dorange_price ->  1.1\n",
      "dorange ->  3\n",
      "dorange_num ->  165\n",
      "dapple ->  2.2\n",
      "dapple_num ->  110\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "print(int(price))\n",
    "\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "print(\"dall_price -> \", dall_price)\n",
    "print(\"dtax -> \", dtax)\n",
    "\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "print(\"dapple_price -> \", dapple_price)\n",
    "print(\"dorange_price -> \", dorange_price)\n",
    "\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "print(\"dorange -> \", int(dorange))\n",
    "print(\"dorange_num -> \", int(dorange_num))\n",
    "\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "print(\"dapple -> \", dapple)\n",
    "print(\"dapple_num -> \", int(dapple_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e9024-d1a3-4886-bc0b-e77af28b35b5",
   "metadata": {},
   "source": [
    "## 5.5 활성화 함수 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb5acd0-6631-411e-b8b5-9107f4ba8e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "\n",
    "    def __init__(self):\n",
    "        # T/F로 구성된 넘파이 배열\n",
    "        # 순전파 입력인 x의 원소 값이 0 이하인 인덱스는 True.\n",
    "        # 그 외 (0보다 큰 원소)는 False.\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9f2c128-c706-4cb7-82a4-3fd479610663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[False,  True],\n",
       "       [ True, False]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array( [[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "\n",
    "mask = ( x <= 0 )\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d4fb0-e660-4f30-a08a-11d5ad0cdf43",
   "metadata": {},
   "source": [
    "## 5.5.2 Sigmoid 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aeaeeb1-a631-4386-b005-d9637110239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # dσ(x)/dx = σ(x)⋅(1−σ(x))\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a959c47-f618-40ea-97fb-bea64455521b",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmax 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f3b9e9c-ab57-4c0f-8cb6-c58284a288da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "X_dot_W = np.array([[0,0,0], [10,10,10]])\n",
    "B = np.array([1,2,3])\n",
    "\n",
    "print(X_dot_W)\n",
    "print(X_dot_W + B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42a524ec-8810-4549-b88c-459a9b2c0f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1,2,3], [4,5,6]])\n",
    "dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16c8c8da-2829-499e-9567-feeb6427e084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# axis=0은 열 방향으로 합을 계산\n",
    "dB = np.sum(dY, axis=0)\n",
    "dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34aa79c8-b0fb-41a1-97cc-a64b5cb1f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    # dout => 이전 레이어로부터 전달된 그래디언트(기울기)\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "    \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1725a-ddeb-41d7-ae4e-de4121a3b811",
   "metadata": {},
   "source": [
    "## 5.6.3 Softmax - with - Loss 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e6eeb3-886c-4691-9191-ce10337bb3a8",
   "metadata": {},
   "source": [
    "Softmax 계층은 입력 (a1,a2,a3)를 정규화하여 (y1,y2,y3)를 출력  <br>\n",
    "Cross Entropy Error 계층은 Softmax의 출력 (y1,y2,y3)와 정답 레이블 (t1,t2,t3)를 받고, 이 데이터들로부터 손실 L을 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b69e9b-d81c-4046-b862-1d9fb44c06a3",
   "metadata": {},
   "source": [
    "예를들어, 정답 레이블이 (0,1,0) 일 때 SoftMax 계층이 (0.3, 0.2, 0.5)를 출력 <br>\n",
    "정답 레이블의 인덱스는 1 <br>\n",
    "그런데 출력에서는 이때의 확률이 0.2에 불과 <br>\n",
    "Softmax 계층의 역전파는 (0.3, -0.8, 0.5)라는 큰 오차를 전파 <br>  --> (0.3-0, 0.2-1, 0.5-0) <br>\n",
    "결과적으로, Softmax 계층의 앞 계층들은 그 큰 오차로부터 더 크게 움직임 (학습 정도가 커짐)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caa6e60e-3221-4cc4-b249-7e87e6290e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None #손실\n",
    "        self.y = None #softmax 출력\n",
    "        self.t = None #정답 레이블 (ont-hot encoding)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        # 역전파 때는 전파하는 값을 배치의 수로 나눠 데이터 \"1개당\" 오차를 앞 계층으로 전파.\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59370bb-5c32-4513-bbd6-6218c38338c5",
   "metadata": {},
   "source": [
    "### 5.7.1 오차역전파법 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe77660-1f00-48fe-b879-23463c2ef4ef",
   "metadata": {},
   "source": [
    "<b> 전제 </b> </br>\n",
    "신경망에는 적응 가능한 가주치와 편향이 있다. <br>\n",
    "가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.<br>\n",
    "신경망 학습은 4단계로 수행된다. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<b> 1단계 - 미니배치 </b> </br>\n",
    "훈련 데이터 중 일부를 무작위로 가져온다. (선별 데이터를 미니배치라고 함) </br>\n",
    "해당 미니배치의 손실 함수 값을 줄이는 것이 목표 <br>\n",
    "\n",
    "<b> 2단계 - 기울기 산출 </b> </br>\n",
    "미니배치 손실 함수 값을 줄이기 위해 각 가중치 매개변수 기울기 구함 <br>\n",
    "기울기는 손실 함수의 값을 가장 작게 하는 방향 제시 <br>\n",
    "\n",
    "<b> 3단계 - 매개변수 갱신 </b> </br>\n",
    "가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
    "\n",
    "<b> 4단계 - 반복 </b> </br>\n",
    "1~3단계를 반복 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d0e4d7-ed8a-4ee3-bd3a-20ff164862d2",
   "metadata": {},
   "source": [
    "오차역전파법이 등장하는 단계는 두 번째인 '기울기 산출' 단계 <br>\n",
    "오차역전파법 이용 시 느린 수치 미분과 달리 기울기를 효율적이고 빠르게 구할 수 있음 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f197471-4303-4a09-a89f-9c630bfc203f",
   "metadata": {},
   "source": [
    "### 5.7.2 오차역전파법을 적용한 신경망 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ee640-df46-408a-9b6e-2f4adcd84705",
   "metadata": {},
   "source": [
    "2층 신경망을 TwoLayerNet 클래스로 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8c98b-8929-4c7b-b7c3-65f1686b30c7",
   "metadata": {},
   "source": [
    "### TwoLayerNet 클래스의 인스턴스 변수\n",
    "\n",
    "\n",
    "<b> params </b> - 딕셔너리 변수로, 신경망의 매개변수 보관 </br>\n",
    "- params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향 <br>\n",
    "- params['W2']은 2번째 층의 가중치, params['b2']는 2번째 층의 편향 <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<b> layers </b> - 순서가 있는 딕셔너리 변수로, 신경망의 계층 보관 </br>\n",
    "- layers['Affine1'], layers['Relu1'], layers['Affine2']와 같이 각 계층을 순서대로 유지\n",
    "\n",
    "<br>\n",
    "\n",
    "<b> lastLayer </b> - 신경망의 마지막 계층\n",
    "- 이 예에서는 SoftmaWithLoss 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb018b-eb1a-4e96-acf8-1024f398ed7f",
   "metadata": {},
   "source": [
    "### TwoLayerNet 클래스의 메소드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327f377-4dee-427e-8638-39ed4b7244b9",
   "metadata": {},
   "source": [
    "<b> __init__(self, input_size, hidden_size, output_size, weight_init_std) </b> <br>\n",
    "- 초기화 수행, 인수는 앞에서부터 입력층 뉴런 수, 은닉층 뉴런 수, 출력층 뉴런 수, 가중치 초기화 시 정규분포의 스케일\n",
    "\n",
    "\n",
    "<b> predict(self,x) </b>\n",
    "- 예측(추론)을 수행, 인수 x는 이미지 데이터\n",
    "\n",
    "<b> loss(self,x,t) </b> </br>\n",
    "- 손실 함수의 값을 구함, 인수 x는 이미지 데이터, t는 정답 레이블 <br>\n",
    "\n",
    "<b> accuracy(self,x,t) </b> <br>\n",
    "- 정확도를 구한다.\n",
    "\n",
    "<b> numerical_gradient(self,x,t) </b> <br>\n",
    "- 가중치 매개변수의 기울기를 수치 미분 방식을 구함 <br>\n",
    "\n",
    "<b> gradient(self,x,t) </b> <br>\n",
    "- 가중치 매개변수의 기울기를 오차역전파법으로 구함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee1a8cbe-47c8-447f-b422-bea6e8074532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "018ca44a-35f8-46ed-8e3d-b928bcf6a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hiden_szie, output_size,\n",
    "                 weight_init_std=0.01):\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size) # randn 함수를 사용하여 평균이 0이고 표준 편차가 1인 정규 분포에서 무작위로 수를 생성\n",
    "                                                                     # 행렬의 크기는 input_size x hidden_size\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = \\\n",
    "            Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Affine2'] = \\\n",
    "            Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1) # 가장 높은 확률을 가진 클래스의 인덱스를 선택\n",
    "        \n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1) # t가 one-hot 인코딩되지 않았을 때, 최댓값의 인덱스로 변환하여 정답 클래스를 추출\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0]) # 예측된 클래스와 정답 클래스를 비교하여 일치하는 개수를 세고, 이를 전체 데이터 포인트 수로 나누어 정확도를 계산\n",
    "\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    # 수치 미분\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W : self.loss(x, t)\n",
    "\n",
    "        grads = {} # 매개변수별로 수치 미분 결과를 저장\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    # 오차역전파\n",
    "    def gradient(self, x, t):\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e99a58-b703-42e2-bf29-042ef6e55e26",
   "metadata": {},
   "source": [
    "### 5.7.3 오차역전파법으로 구한 기울기 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15519c23-fd4e-46aa-86c8-bf200250c662",
   "metadata": {},
   "source": [
    "기울기 구하는 2가지 방법 <br>\n",
    "1. 수치 미분 <br>\n",
    "2. 해석적으로 수식을 풀어 구하는 방법 <br>\n",
    "\n",
    "후자인 해석적 방법은 오차역전파법 <br>\n",
    "\n",
    "수치 미분은 느리다. 장점은 구현이 쉬움 <br>\n",
    "\n",
    "오차역전파법을 제대로 구현했는지 검증하기 위해 수치 미분 사용 <br>\n",
    "두 방식으로 구한 기울기가 거의 같음을 확인하는 작업을 기울기 확인(gradient check)이라고 함 <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f24d314e-bda0-4ae8-9589-b23786069d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66b5059c-d0df-40f2-ae86-722bdff290d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:1.9559642116786304e-10\n",
      "b1:1.1096058585483406e-09\n",
      "W2:7.198160315272096e-08\n",
      "b2:1.434017297463619e-07\n"
     ]
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "(X_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "X_batch = X_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(X_batch, t_batch)\n",
    "grad_backprop = network.gradient(X_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c871de-8455-41a5-9f9f-2b3191b03ad8",
   "metadata": {},
   "source": [
    "훈련 데이터 일부를 수치 미분으로 구한 기울기와 오차역전파법으로 구한 기울기의 오차 확인 <br>\n",
    "각 가중치 매개변수의 차이의 절댓값을 구하고, 이를 평균한 값이 오차 <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812d91b-5b9b-442c-b594-3235d2301c87",
   "metadata": {},
   "source": [
    "### 5.7.4 오차역전파법을 사용한 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84e4752a-dcaf-416a-9ad0-060c72723dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfe798b0-402f-4fbc-b9ec-73906e03f4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10441666666666667 0.1028\n",
      "0.7877 0.7883\n",
      "0.8765 0.8795\n",
      "0.8979333333333334 0.9009\n",
      "0.90785 0.9111\n",
      "0.9149 0.9169\n",
      "0.9194 0.9215\n",
      "0.9250166666666667 0.9252\n",
      "0.9278833333333333 0.9283\n",
      "0.9308833333333333 0.9315\n",
      "0.9346666666666666 0.9333\n",
      "0.93735 0.938\n",
      "0.9398833333333333 0.9395\n",
      "0.9419833333333333 0.9408\n",
      "0.9435 0.9422\n",
      "0.94555 0.9438\n",
      "0.9469333333333333 0.9443\n"
     ]
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "(X_train, t_train), (X_test, t_tets) = \\\n",
    "    load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = X_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    X_batch = X_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 오차역전파법으로 기울기 계산.\n",
    "    grad = network.gradient(X_batch, t_batch)\n",
    "\n",
    "    # 목적: 손실 함수를 최소화하는 방향으로 매개변수를 조정\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        # 경사 하강법은 현재 매개변수에서 기울기를 빼는 것\n",
    "        network.params[key] -= learning_rate * grad[key] # grad[key]는 해당 매개변수에 대한 기울기\n",
    "\n",
    "    # 현재 미니배치에 대한 손실을 계산\n",
    "    # 신경망이 현재 매개변수로 얼마나 좋은 예측을 하는지를 나타내는 값\n",
    "    loss = network.loss(X_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 현재 반복 횟수 i가 에폭의 배수일 때 (즉, 에폭이 끝났을 때) 아래 코드 블록을 실행\n",
    "    # 에폭(epoch)은 전체 훈련 데이터셋에 대해 한 번 학습을 완료한 상태\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(X_train, t_train)\n",
    "        test_acc = network.accuracy(X_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a843e2ea-e54e-4159-8251-cdf5dc392bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
