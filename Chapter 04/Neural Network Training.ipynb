{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba4279ed-43b3-401d-bc91-9668b3df7d70",
   "metadata": {},
   "source": [
    "## 4.2 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64d2eec-0637-4b5e-a385-d6d8c47e1b26",
   "metadata": {},
   "source": [
    "신경망은 '하나의 특정한 지표'를 기준으로 최적의 매개변수 값을 탐색 <br>\n",
    "이 지표가 신겸망에서는 '손실 함수(loss function)' 이라고 칭한다. <br>\n",
    "손실 함수는 임의의 함수를 사용할 수도 있지만 일반적으로 오차제곱합과 교차 엔트로피 오차를 사용 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa0ecf2-d8e3-4a36-b724-d8b0a8ba0aa4",
   "metadata": {},
   "source": [
    "손실 함수는 신경망 성능의 '나쁨'을 나타내는 지표<br>\n",
    "현재 신경망이 훈련 데이터를 엄라나 잘 처리하지 '못'하느냐를 측정 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95a2e1f-eb70-4490-bb80-820f24e2534a",
   "metadata": {},
   "source": [
    "## 4.2.1 오차제곱합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1d97f9-d2bb-41a7-87db-44ac42200de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지가 해당 인덱스일 확률 의미. (ex. 이미지가 '1'일 확률은 0.05)\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "# 정답 레이블 (one-hot encoding)\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "852f56b6-3fb7-44af-95cf-82f1bdd19e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1053b3a-a2bf-45f1-9672-0ae532dcc2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0c487b-436e-478a-be37-e5b06e6d96d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '7'일 확률이 가장 높다고 추정 (0.6)\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fe78fa-397a-4757-8b0b-018918030bee",
   "metadata": {},
   "source": [
    "## 4.2.2 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d10d7-6512-48b0-bc60-2851108f61a1",
   "metadata": {},
   "source": [
    "교차 엔트포리의 오차는 정답일 때의 출력이 전체 값을 정한다. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "817cfb89-506a-47f6-b952-96f275b37d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    # log0은 inf이므로, 아주 작은 값인 delta를 더한다.\n",
    "    return -np.sum(t * np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4297850-1e7f-4335-880f-502979cd3e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01fa737d-a78d-43cd-bf89-a264c7242f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b2e11-da5a-4ff8-8f6e-88b87faa901c",
   "metadata": {},
   "source": [
    "## 4.2.3 미니배치 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4d7516f-1458-4240-95ba-72501ca84383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (60000, 784)\n",
      "y_train shape =  (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist(one_hot_label=True, normalize=False)\n",
    "print('X_train shape = ',X_train.shape)\n",
    "print('y_train shape = ',y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b65c6d4-5f06-481b-bd41-0ac5ea1b79b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위 10장 추출 (minibatch)\n",
    "train_size = X_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = X_train[batch_mask]\n",
    "y_batch = y_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93af5f32-260a-4872-a22c-8863c415879c",
   "metadata": {},
   "source": [
    "## 4.2.4 (배치용) 교차 엔트로피 오차 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2776d2fe-6456-41e2-837e-a8afa00501d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entrophy_error(y, t):\n",
    "    # y가 1차원 즉, 데이터 하나라면 reshape 함수로 데이터 형상 변환.\n",
    "    if y.ndin == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d2605da-670b-4dd2-9422-647d8c857bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 레이블이 원-핫 인코딩이 아닌 경우\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fa9efd-9106-4fff-a55a-c4b5e15b6200",
   "metadata": {},
   "source": [
    "## 4.2.5 왜 손실 함수를 설정하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018c263-cb90-4052-a386-0cd1b8ddc199",
   "metadata": {},
   "source": [
    "신경망 학습에서의 '미분'의 역할에 주목하자 <br>\n",
    "신경망 학습에서는 최적의 매개변수(가중치와 편향)를 탐색 할 때 손실함수의 값을 가능한 작게 하는 매개변수 값을 찾는 것 <br>\n",
    "매개변수의 미분(정확히는 기울기)을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정 반복 <br>\n",
    "가중치 매개변수의 손실 함수의 미분이란 '가중치 매개변수의 값을 아주 조금 변화 시켰을 때, 손실 함수가 어떻게 변하나' 라는 의미 <br>\n",
    "<br>\n",
    "정확도를 지표로 삼아서는 안되는 이유 -> 대부분의 장소에서 미분값이 0이 될 시 매개변수를 갱신할 수 없음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdef05c1-da91-4e7b-969f-45cc593206b4",
   "metadata": {},
   "source": [
    "## 4.4 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f37af9d-844c-4a81-8c71-660c692bd78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 아주 작은 값\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x + h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x - h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcad037-9781-4e8c-8929-2bf8c660ce2a",
   "metadata": {},
   "source": [
    "## 4.4.1 경사법(경사 하강법)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c128f-045b-43b6-838f-b4f374534b3e",
   "metadata": {},
   "source": [
    "기계학습 문제 대부분은 학습 단계에서 최적의 매개변수를 찾아낸다 <br>\n",
    "여기에서 최적이란 손실 함수가 최솟값이 될 때의 매개변수 값. <br>\n",
    "그러나 주의할 점은 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 '기울기'라는 것 <br>\n",
    "해당 기울기가 가리키는 곳에 정말 함수의 최솟값이 존재하는지 보장 X <br>\n",
    "\n",
    "<br>\n",
    "경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동 <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5d4f320-176f-42b6-b488-30d4dfeb0528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr = 0.01, step_num = 100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd612e2b-bed2-41f5-9d62-0ec266d7bed2",
   "metadata": {},
   "source": [
    "## 4.4.2 신경망에서의 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a78b363f-f72f-4b37-b94b-bae045af6a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1ad8ba7-5722-495e-b2db-07bdf5a6e68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07361655  0.94504201 -0.63466312]\n",
      " [ 0.99411904  0.01062444  1.68236377]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28e3ecb4-2c00-4761-9cdd-032a1c73e6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93887707 0.5765872  1.13332952]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef6d2601-38f3-452b-ab23-bdabba443553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a08ac157-d48d-4b9c-aaac-f9953892ed20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8739499459263292"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25d4ca81-e91c-412f-a980-8bc50803efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda w : net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ad664fc-3eeb-4849-8c2a-c00082f725a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20613407,  0.14348592, -0.34961999],\n",
       "       [ 0.3092011 ,  0.21522888, -0.52442998]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f85ca-1fb7-4601-880f-cb4f9e4cf4b5",
   "metadata": {},
   "source": [
    "## 학습 알고리즘 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7688bc-cda8-4257-9b0e-4a4714de0144",
   "metadata": {},
   "source": [
    "<b>전제</b> <br>\n",
    "신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라고 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd3dd7-1b45-4793-8d14-de4af88ad9bf",
   "metadata": {},
   "source": [
    "<b>1단계 - 미니배치</b> <br>\n",
    "훈련 데이터 중 일부를 무작위로 가져온다. <br>\n",
    "이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실함수 값을 줄이는 것이 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bde07e-f6c1-4f2f-8cf6-32941847555d",
   "metadata": {},
   "source": [
    "<b>2단계 - 기울기 산출</b> <br>\n",
    "미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. <br>\n",
    "기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ccf12-ee30-4057-8140-3caacfad2dec",
   "metadata": {},
   "source": [
    "<b>3단계 - 매개변수 갱신</b> <br>\n",
    "가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47acaf6-e9f7-40f4-bdb1-7cf1f51f7ec2",
   "metadata": {},
   "source": [
    "<b>4단계 - 반복</b> <br>\n",
    "1~3단계를 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c1ac9-c7d8-4481-9000-745493e58f1a",
   "metadata": {},
   "source": [
    "## 4.5.1 2층 신경망 클래스 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1616b9b3-5e40-4fd7-8bd7-a021c581437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b9c3d51-d605-427e-b515-51444585f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                 weight_init_std = 0.01):\n",
    "\n",
    "        # 가중치 초기화\n",
    "        # 정규분포를 따르는 난수로, 표준편차는 0으로 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    # 손실 함수 계산\n",
    "    # 예측 결과와 정답 레이블을 바탕으로 교차 엔트로피 측정.\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    def accuracy(self, x):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "\n",
    "        accuracy = np.sum( y == t ) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # 각 매개변수 기울기 측정\n",
    "    def numerical_gradient(self, x, t):\n",
    "        # 함수 형태로 정의\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2204c972-645f-4bde-ac55-ba33f67f08ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df957e55-1d56-4ea6-ab04-b255a19f6c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측처리\n",
    "x = np.random.rand(100, 784)\n",
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5939111-ec57-483b-a7e8-db563e700592",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m784\u001b[39m)\n\u001b[1;32m      3\u001b[0m t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 기울기 계산\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 49\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     46\u001b[0m loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[1;32m     48\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 49\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     51\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/DeepLearning_Study/Chapter 04/common/gradient.py:43\u001b[0m, in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m tmp_val \u001b[38;5;241m=\u001b[39m x[idx]\n\u001b[1;32m     42\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m tmp_val \u001b[38;5;241m+\u001b[39m h\n\u001b[0;32m---> 43\u001b[0m fxh1 \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# f(x+h)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m tmp_val \u001b[38;5;241m-\u001b[39m h \n\u001b[1;32m     46\u001b[0m fxh2 \u001b[38;5;241m=\u001b[39m f(x) \u001b[38;5;66;03m# f(x-h)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 46\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# 함수 형태로 정의\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     49\u001b[0m     grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[22], line 31\u001b[0m, in \u001b[0;36mTwoLayerNet.loss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m---> 31\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy_error(y, t)\n",
      "Cell \u001b[0;32mIn[22], line 20\u001b[0m, in \u001b[0;36mTwoLayerNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m W1, W2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m b1, b2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 20\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n\u001b[1;32m     21\u001b[0m z1 \u001b[38;5;241m=\u001b[39m sigmoid(a1)\n\u001b[1;32m     22\u001b[0m a2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(z1, W2) \u001b[38;5;241m+\u001b[39m b2\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 기울기 계산\n",
    "x = np.random.rand(100, 784)\n",
    "t = np.random.rand(100, 10)\n",
    "\n",
    "grads = net.numerical_gradient(x, t) # 기울기 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f3de1-52b2-4a61-bd58-87ba46a6722f",
   "metadata": {},
   "source": [
    "## 4.5.2 미니배치 학습 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ebf7b7-708c-4467-83ed-75f32428371b",
   "metadata": {},
   "source": [
    "훈련 데이터 중 일부를 무작위로 꺼내고(미니배치), 그 매니배치에 대해서 경사법으로 매개변수를 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929a0164-3138-474d-be7a-dc81acc094c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24843b2e-f221-4bfc-af7b-f96e97147fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = \\\n",
    "    load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "# 60,000개의 샘플, 하나의 샘플 당 784차원 (28x28 pixel)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000 # 반복 횟수\n",
    "train_size = X_train.shape[0]\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    \n",
    "    x_batch = X_train[batch_mask]\n",
    "    \n",
    "    # 정답 레이블\n",
    "    y_batch = y_train[batch_mask]\n",
    "\n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, y_batch)\n",
    "\n",
    "   # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, y_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297221f1-523a-46b9-9c3c-c15b98119b7b",
   "metadata": {},
   "source": [
    "## 4.5.3 시험 데이터로 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b13a6-a69f-45b7-96ae-d54c96d9a161",
   "metadata": {},
   "source": [
    "손실 함수의 값이란, '훈련 데이터의 미니배치에 대한 손실 함수'의 값 <br>\n",
    "신경망 학습에서는 훈련 데이터 외의 데이터를 올바르게 인식하는지를 확인해야 한다. <br>\n",
    "즉, 오버피팅 유무를 확인해야 한다. <br> \n",
    "오버피팅이 되었다는 것은 훈련 데이터에 포함된 이미지만 제대로 구별하고, 그렇지 않은 이미지는 식별할 수 없다는 뜻 <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7477cc80-981c-46df-8721-d4e8ea121989",
   "metadata": {},
   "source": [
    "<b> 에폭(epoch) </b>\n",
    "에폭은 하나의 단위이다. 1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당한다. <br>\n",
    "예를 들어 10,000개를 100개의 미니배치로 학습할 경우, 확률적 경사 하강법을 100회 반복하면 모든 훈련 데이터를 소진한 게 된다. <br>\n",
    "이 경우 100회가 1에폭이 된다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123c17a-93d2-4154-8a9c-41a3e5a75445",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 미니배치로 쪼개서 훈련 시키는 횟수 --> EX:(600000/ 100 ,1) 중 큰 숫자가 에폭 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = X_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "\n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, y_batch)\n",
    "\n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, y_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 매 에폭(epoch)마다 훈련 데이터와 테스트 데이터에 대한 정확도를 계산\n",
    "    # iter_per_epoch는 한 번의 에폭을 처리하는데 필요한 반복 횟수 (전체 데이터를 모두 한 바퀴 돌았을 때)     \n",
    "    # EX) 100000 / 100(미니배치 사이즈)  ==> 한 에폭: 100\n",
    "    # 즉, 매 에폭마다 실행\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(X_train, y_train)\n",
    "        test_acc = network.accuracy(X_test, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"Epoch {0}: train acc, test acc | {1}, {2}\".format(i // iter_per_epoch, train_acc, test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bbe85a-e35f-45c3-9b48-28143565b87a",
   "metadata": {},
   "source": [
    "## 4.6 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3c7ab-f0e3-48ec-9d6c-5d4c18587224",
   "metadata": {},
   "source": [
    "1. 신경망 학습은 손실 함수를 지표로, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신한다.<br>\n",
    "2. 가중치 매개변수 갱신 시, 가중치 매개변수의 '기울기'를 이용하고 기울어진 방향으로 가중치의 값을 갱신하는 작업을 반복한다. <br>\n",
    "3. 아주 작은 값을 주었을 때의 차분으로 미분하는 것을 수치 미분이라 한다. <br>\n",
    "4. 수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있다. <br>\n",
    "5. 수치 미분 -> 계산에는 시간이 걸리지만 구현은 간단. <br>\n",
    "6. 오차역전파법은 기울기를 고속으로 구할 수 있다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cea815-748c-40c3-8e3b-05064e562881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
